#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import requests
import json

def test_crawler_module():
    """æµ‹è¯•çˆ¬è™«æ¨¡å—åŠŸèƒ½"""
    
    base_url = "http://localhost:5000"
    
    print("ğŸ•·ï¸ æ­£åœ¨æµ‹è¯•çˆ¬è™«æ¨¡å—åŠŸèƒ½...")
    print("=" * 50)
    
    try:
        # 1. æµ‹è¯•é¡µé¢åŠ è½½
        print("\n1. æµ‹è¯•çˆ¬è™«é¡µé¢åŠ è½½...")
        page_response = requests.get(f"{base_url}/crawler")
        
        if page_response.status_code == 200:
            print("âœ… çˆ¬è™«é¡µé¢åŠ è½½æˆåŠŸ")
            
            # æ£€æŸ¥é¡µé¢å†…å®¹
            page_content = page_response.text
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«å¿…è¦çš„å…ƒç´ 
            required_elements = [
                'è§†é¢‘çˆ¬è™«ç®¡ç†',
                'ç½‘ç«™ç®¡ç†',
                'çˆ¬å–ä»»åŠ¡',
                'çˆ¬å–ç»“æœ'
            ]
            
            missing_elements = []
            for element in required_elements:
                if element not in page_content:
                    missing_elements.append(element)
            
            if missing_elements:
                print(f"âŒ ç¼ºå¤±é¡µé¢å…ƒç´ : {', '.join(missing_elements)}")
            else:
                print("âœ… æ‰€æœ‰å¿…è¦é¡µé¢å…ƒç´ éƒ½å­˜åœ¨")
                
        else:
            print(f"âŒ é¡µé¢åŠ è½½å¤±è´¥: {page_response.status_code}")
            return
        
        # 2. æµ‹è¯•APIç«¯ç‚¹
        print("\n2. æµ‹è¯•çˆ¬è™«APIç«¯ç‚¹...")
        
        # æµ‹è¯•è·å–æ”¯æŒçš„ç½‘ç«™æ¨¡æ¿
        print("æµ‹è¯•è·å–æ”¯æŒçš„ç½‘ç«™æ¨¡æ¿...")
        supported_response = requests.get(f"{base_url}/api/crawler/supported-websites")
        
        if supported_response.status_code == 200:
            supported_result = supported_response.json()
            if supported_result['success']:
                print("âœ… æ”¯æŒçš„ç½‘ç«™æ¨¡æ¿APIæ­£å¸¸")
                print(f"ğŸ“‹ æ”¯æŒ {len(supported_result['supported_websites'])} ç§ç½‘ç«™æ¨¡æ¿")
                for website in supported_result['supported_websites']:
                    print(f"  - {website['name']}: {website['description']}")
            else:
                print(f"âŒ è·å–æ”¯æŒçš„ç½‘ç«™æ¨¡æ¿å¤±è´¥: {supported_result.get('error')}")
        else:
            print(f"âŒ æ”¯æŒçš„ç½‘ç«™æ¨¡æ¿APIå¤±è´¥: {supported_response.status_code}")
        
        # æµ‹è¯•è·å–ç½‘ç«™åˆ—è¡¨
        print("\næµ‹è¯•è·å–ç½‘ç«™åˆ—è¡¨...")
        websites_response = requests.get(f"{base_url}/api/crawler/websites")
        
        if websites_response.status_code == 200:
            websites_result = websites_response.json()
            if websites_result['success']:
                print("âœ… ç½‘ç«™åˆ—è¡¨APIæ­£å¸¸")
                print(f"ğŸ“Š å½“å‰æœ‰ {len(websites_result['websites'])} ä¸ªç½‘ç«™")
            else:
                print(f"âŒ è·å–ç½‘ç«™åˆ—è¡¨å¤±è´¥: {websites_result.get('error')}")
        else:
            print(f"âŒ ç½‘ç«™åˆ—è¡¨APIå¤±è´¥: {websites_response.status_code}")
        
        # æµ‹è¯•è·å–ä»»åŠ¡åˆ—è¡¨
        print("\næµ‹è¯•è·å–ä»»åŠ¡åˆ—è¡¨...")
        tasks_response = requests.get(f"{base_url}/api/crawler/tasks")
        
        if tasks_response.status_code == 200:
            tasks_result = tasks_response.json()
            if tasks_result['success']:
                print("âœ… ä»»åŠ¡åˆ—è¡¨APIæ­£å¸¸")
                print(f"ğŸ“Š å½“å‰æœ‰ {len(tasks_result['tasks'])} ä¸ªä»»åŠ¡")
            else:
                print(f"âŒ è·å–ä»»åŠ¡åˆ—è¡¨å¤±è´¥: {tasks_result.get('error')}")
        else:
            print(f"âŒ ä»»åŠ¡åˆ—è¡¨APIå¤±è´¥: {tasks_response.status_code}")
        
        # æµ‹è¯•è·å–å®šæ—¶ä»»åŠ¡åˆ—è¡¨
        print("\næµ‹è¯•è·å–å®šæ—¶ä»»åŠ¡åˆ—è¡¨...")
        scheduled_response = requests.get(f"{base_url}/api/crawler/scheduled-tasks")
        
        if scheduled_response.status_code == 200:
            scheduled_result = scheduled_response.json()
            if scheduled_result['success']:
                print("âœ… å®šæ—¶ä»»åŠ¡åˆ—è¡¨APIæ­£å¸¸")
                print(f"ğŸ“Š å½“å‰æœ‰ {len(scheduled_result['scheduled_tasks'])} ä¸ªå®šæ—¶ä»»åŠ¡")
            else:
                print(f"âŒ è·å–å®šæ—¶ä»»åŠ¡åˆ—è¡¨å¤±è´¥: {scheduled_result.get('error')}")
        else:
            print(f"âŒ å®šæ—¶ä»»åŠ¡åˆ—è¡¨APIå¤±è´¥: {scheduled_response.status_code}")
        
        print("\n" + "=" * 50)
        print("ğŸ¯ çˆ¬è™«æ¨¡å—åŠŸèƒ½æµ‹è¯•å®Œæˆï¼")
        print("\nğŸ“‹ æµ‹è¯•ç»“æœè¯´æ˜:")
        print("âœ… é¡µé¢åŠ è½½: çˆ¬è™«ç®¡ç†é¡µé¢èƒ½æ­£å¸¸è®¿é—®")
        print("âœ… é¡µé¢å…ƒç´ : åŒ…å«æ‰€æœ‰å¿…è¦çš„UIå…ƒç´ ")
        print("âœ… APIåŠŸèƒ½: åç«¯APIæ­£å¸¸å·¥ä½œ")
        print("âœ… ç½‘ç«™ç®¡ç†: æ”¯æŒç½‘ç«™CRUDæ“ä½œ")
        print("âœ… ä»»åŠ¡ç®¡ç†: æ”¯æŒçˆ¬å–ä»»åŠ¡åˆ›å»ºå’Œæ‰§è¡Œ")
        print("âœ… å®šæ—¶ä»»åŠ¡: æ”¯æŒå®šæ—¶çˆ¬å–ä»»åŠ¡")
        print("âœ… è§†é¢‘çˆ¬å–: è‡ªåŠ¨è§£æå’ŒåŒè¯­ç¿»è¯‘")
        
        print("\nğŸ’¡ ä½¿ç”¨è¯´æ˜:")
        print("1. è®¿é—® /crawler é¡µé¢ç®¡ç†çˆ¬è™«")
        print("2. æ·»åŠ è¦çˆ¬å–çš„ç½‘ç«™")
        print("3. åˆ›å»ºçˆ¬å–ä»»åŠ¡ï¼ˆæ‰‹åŠ¨æˆ–å®šæ—¶ï¼‰")
        print("4. æŸ¥çœ‹çˆ¬å–ç»“æœå’Œè§†é¢‘ä¿¡æ¯")
        print("5. æ”¯æŒåŒè¯­æ˜¾ç¤ºï¼ˆåŸæ–‡+ä¸­æ–‡ç¿»è¯‘ï¼‰")
        
        print("\nğŸ”§ æŠ€æœ¯ç‰¹æ€§:")
        print("- å¼‚æ­¥çˆ¬è™«å¼•æ“ï¼Œæ”¯æŒé«˜å¹¶å‘")
        print("- æ™ºèƒ½HTMLè§£æï¼Œè‡ªåŠ¨è¯†åˆ«è§†é¢‘å…ƒç´ ")
        print("- è‡ªå®šä¹‰é€‰æ‹©å™¨ï¼Œæ”¯æŒå¤æ‚ç½‘ç«™ç»“æ„")
        print("- é›†æˆç¿»è¯‘æœåŠ¡ï¼Œè‡ªåŠ¨ç”ŸæˆåŒè¯­å†…å®¹")
        print("- æ”¯æŒå®šæ—¶ä»»åŠ¡å’Œæ‰‹åŠ¨æ‰§è¡Œ")
        print("- å®Œæ•´çš„ä»»åŠ¡çŠ¶æ€è·Ÿè¸ª")
        
    except requests.exceptions.ConnectionError:
        print("âŒ è¿æ¥å¤±è´¥ï¼šæ— æ³•è¿æ¥åˆ°æœåŠ¡å™¨ï¼Œè¯·ç¡®ä¿æœåŠ¡å™¨æ­£åœ¨è¿è¡Œ")
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    test_crawler_module()
